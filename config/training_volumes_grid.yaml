# Hyperparameters
batch_size: 20  # batch size
lr: 1e-5  # learning rate
patience: 5  # patience for  early stopping
weight_decay: 1.e-11  # weight decay factor

# Network structure
latent_channels: 10 # number of channels of the volume between the gnn and the cnn
latent_resolution: [30, 30] # It can be the latent resolution or grid dimension if the node lattice is used
dropout: 0.  # dropout for the whole network?
residual: false # residual connections for the whole network?

networks_params:
  gnn_model:
    net: 'gat'
    alt_feats: 'clusters'  # choose the alternative for the visual features: img, hue, mean, clusters
    alt: '1' # choose betwen grid 1 or not grid 0
    gnn_hidden: [25, 12]  # dimension of the gnn hidden layers
    num_heads: [20, 11]  # size of the attention heads
    activation: 'leaky_relu'  # activation between the layers
    final_activation: 'tanh' # activation of the final layer
    alpha: 0.2892912  # attention parameter
    attn_drop: 0.  # attention drop
  cnn_model:
    cnn_hidden: [50, 100]  # dimension of the cnn hidden layers
    depth_output: 128  # number of chanels at the output
    scale_factors: [4, 2, 2]  # scale factors for each of the upsampling layers
    kernel_sizes: [7, 6, 3]
    strides: [5, 4, 3]
    paddings: [7, 5, 2]
    activation: 'leaky_relu'  # activation between the layers
    final_activation: 'tanh'  # activation of the final layer
